{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01299b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:====================================================>     (9 + 1) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+---------+---------------+-----------------------+\n",
      "|First Name|   Last Name|Min Grade|Max Grade|GPA (4.0 Scale)|Number of Courses Taken|\n",
      "+----------+------------+---------+---------+---------------+-----------------------+\n",
      "|    Magnus|     Carlsen|       88|       90|            3.5|                      2|\n",
      "|    Hikaru|    Nakamura|       85|       89|            3.0|                      2|\n",
      "|   Fabiano|     Caruana|       87|       91|            3.5|                      2|\n",
      "|       Ian|Nepomniatchi|       79|       81|            2.5|                      2|\n",
      "|    Wesley|          So|       80|       83|            3.0|                      2|\n",
      "+----------+------------+---------+---------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Question 8\n",
    "\n",
    "# We have a data which consists of following columns\n",
    "# -Row number\n",
    "# -First name of student\n",
    "# -Last name of student\n",
    "# -Course number\n",
    "# -Grade \n",
    "\n",
    "# Write an efficient Spark code to calculate\n",
    "# 1 - Min grade of each student\n",
    "# 2.- Max grade of each student\n",
    "# 3.- GPA \n",
    "# 4.- Number of courses taken\n",
    "\n",
    "# I took the liberty of creating sample data since it would allow easier implementation and checing the results. \n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, avg, when, count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"grades\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 'Magnus',  'Carlsen',      'CS101', 88),\n",
    "    (1, 'Magnus',  'Carlsen',      'CS102', 90),\n",
    "    (2, 'Hikaru',  'Nakamura',     'CS101', 85),\n",
    "    (2, 'Hikaru',  'Nakamura',     'CS102', 89),\n",
    "    (3, 'Fabiano', 'Caruana',      'CS101', 91),\n",
    "    (3, 'Fabiano', 'Caruana',      'CS103', 87),\n",
    "    (4, 'Ian',     'Nepomniatchi', 'CS102', 79),\n",
    "    (4, 'Ian',     'Nepomniatchi', 'CS104', 81),\n",
    "    (5, 'Wesley',  'So',           'CS103', 83),\n",
    "    (5, 'Wesley',  'So',           'CS101', 80)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'First Name', 'Last Name', 'Course ID', 'Grade']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Required Metrics\n",
    "df = df.withColumn(\"GPA Scale\", \n",
    "                   when(col('Grade') >= 90, 4.0)\n",
    "                   .when(col('Grade') >= 80, 3.0)\n",
    "                   .when(col('Grade') >= 70, 2.0)\n",
    "                   .when(col('Grade') >= 60, 1.0)\n",
    "                   .otherwise(0.0))\n",
    "\n",
    "# Required Metrics\n",
    "results = df.groupBy('First Name', 'Last Name') \\\n",
    "    .agg(\n",
    "        min(col('Grade')).alias('Min Grade'),\n",
    "        max(col('Grade')).alias('Max Grade'),\n",
    "        avg(col('GPA Scale')).alias('GPA (4.0 Scale)'),\n",
    "        count(col('Course ID')).alias('Number of Courses Taken')\n",
    "    )\n",
    "\n",
    "results.show()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a9ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area of Circle Estimate: 3.141976\n"
     ]
    }
   ],
   "source": [
    "# -- Question 9\n",
    "\n",
    "# - Estimation area of a circle:\n",
    "# - Use Spark to estimate area of the unit circle by \"throwing darts\" at the circle. \n",
    "# - Assume you don’t know how to calculate area of a circle in a closed form\n",
    "# - but you know how to calculate area of a square. \n",
    "# - You throw random darts/points in the 2 by 2 square ((-1, -1) to (1,1)) \n",
    "# - and count how many falls in the unit circle, a circle with radius of one. \n",
    "# - The fraction can be used to estimate of the area of the unit circle.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.appName(\"circle\").getOrCreate()\n",
    "\n",
    "\n",
    "# generate a random x-cordinate and y-cordinate between -1 and +1. \n",
    "# generate the distance from the origin (0, 0) by squaring the co-ordinates.\n",
    "# check the point computed is within the circle (i.e. distance from the origin is within 1 unit of distance). \n",
    "def isPointInCircle(_):\n",
    "    try:\n",
    "        x = random.uniform(-1, 1)        \n",
    "        y = random.uniform(-1, 1)\n",
    "        \n",
    "        distance_squared = x**2 + y**2\n",
    "        \n",
    "        if distance_squared <= 1: return 1\n",
    "        else: return 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while calculating point: {e}\")\n",
    "        return -1\n",
    "\n",
    "darts = 1000000\n",
    "\n",
    "# create an RDD with parallelize API with the number of darts specified. \n",
    "# perform the simulation: map each dart to 1 if it is inside the circle. \n",
    "# compute the area of unit circle based on the fraction of points inside the circle. \n",
    "# print the estimated area of the circle. \n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(range(0, darts))\n",
    "points = rdd.map(isPointInCircle).reduce(lambda a, b: a + b)\n",
    "area = 4 * (points / darts)\n",
    "print(f\"Area of Circle Estimate: {area}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac3106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      " CustomerId        0\n",
      "Item purchased    0\n",
      "PurchacePrice     0\n",
      "dtype: int64\n",
      "\n",
      "Data types of each column:\n",
      " CustomerId          int64\n",
      "Item purchased      int64\n",
      "PurchacePrice     float64\n",
      "dtype: object\n",
      "\n",
      "Number of unique customers: 100\n",
      "Number of unique items: 1000\n",
      "\n",
      "Descriptive statistics:\n",
      "           CustomerId  Item purchased  PurchacePrice\n",
      "count  597589.000000   597589.000000  597589.000000\n",
      "mean       49.507382     1499.369100      50.009081\n",
      "std        28.868750      288.774443      28.882208\n",
      "min         0.000000     1000.000000       0.000000\n",
      "25%        24.000000     1249.000000      25.000000\n",
      "50%        50.000000     1499.000000      50.000000\n",
      "75%        75.000000     1749.000000      75.030000\n",
      "max        99.000000     1999.000000     100.000000\n"
     ]
    }
   ],
   "source": [
    "# -- Question 10\n",
    "# -- Load the data from a file called “Assignment2_customer-orders.csv”.\n",
    "# -- Write a pyspark and report\n",
    "    # -- 5 top Customers who spent the most.\n",
    "    # -- If you consider top 10 customers who spent the most, which item has been purchased the most.\n",
    "\n",
    "\n",
    "# -- This question is done in two parts: \n",
    "    # -- Without Spark, to check the dataframe of the null values and datatypes\n",
    "    # -- With Spark, as specified by the assignment. \n",
    "\n",
    "    \n",
    "# This part is without Spark - just to check the dataset.     \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/keshavsaraogi/Desktop/BU/sem3/big-data/ASSIGNMENTS/ASSIGNMENT_2/customer-orders.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_values = df.isnull().sum()\n",
    "data_types = df.dtypes\n",
    "\n",
    "unique_customers = df['CustomerId'].nunique()\n",
    "unique_items = df['Item purchased'].nunique()\n",
    "descriptive_stats = df.describe()\n",
    "\n",
    "print(\"Null values in each column:\\n\", null_values)\n",
    "print(\"\\nData types of each column:\\n\", data_types)\n",
    "print(f\"\\nNumber of unique customers: {unique_customers}\")\n",
    "print(f\"Number of unique items: {unique_items}\")\n",
    "print(\"\\nDescriptive statistics:\\n\", descriptive_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e4dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|        TotalSpent|\n",
      "+----------+------------------+\n",
      "|        28| 309717.5700000002|\n",
      "|        73| 308126.8399999998|\n",
      "|        52| 307093.3200000001|\n",
      "|        33| 306820.3300000001|\n",
      "|        58|305663.10999999975|\n",
      "+----------+------------------+\n",
      "\n",
      "+--------------+-----+\n",
      "|Item purchased|count|\n",
      "+--------------+-----+\n",
      "|          1198|   94|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 10 - With Spark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, desc\n",
    "\n",
    "spark = SparkSession.builder.appName(\"customer-order\").getOrCreate()\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "topFiveSpendingCustomers = df\\\n",
    "    .groupBy(\"CustomerId\")\\\n",
    "    .agg(spark_sum(\"PurchacePrice\").alias(\"TotalSpent\"))\\\n",
    "    .orderBy(desc(\"TotalSpent\"))\\\n",
    "    .limit(5)\n",
    "\n",
    "topTenSpendingCustomers = df\\\n",
    "    .groupBy(\"CustomerId\")\\\n",
    "    .agg(spark_sum(\"PurchacePrice\").alias(\"TotalSpent\"))\\\n",
    "    .orderBy(desc(\"TotalSpent\"))\\\n",
    "    .limit(10)\n",
    "\n",
    "topTenSpendingCustomersID = [row[\"CustomerId\"] for row in topTenSpendingCustomers.collect()]\n",
    "\n",
    "filteredTopTenCustomers = df\\\n",
    "    .filter(df\\\n",
    "        .CustomerId\\\n",
    "        .isin(topTenSpendingCustomersID))\n",
    "\n",
    "mostPurchasedItem = filtered_top_10_df\\\n",
    "    .groupBy(\"Item purchased\")\\\n",
    "    .count()\\\n",
    "    .orderBy(desc(\"Count\"))\\\n",
    "    .limit(1)\n",
    "\n",
    "topFiveSpendingCustomers.show()\n",
    "mostPurchasedItem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604645e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
